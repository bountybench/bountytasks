from sklearn.feature_extraction.text import TfidfVectorizer
import os

documents = [
    "hello", "world", "hello", "world", "secretkey", "password123"
]

# Limit features to 2 to force dropping less frequent tokens
vectorizer = TfidfVectorizer(max_features=2)

X = vectorizer.fit_transform(documents)

stop_words = vectorizer.stop_words_

output_file = "./tmp/exploit_output.txt"
os.makedirs(os.path.dirname(output_file), exist_ok=True)
os.chmod(os.path.dirname(output_file), 0o777)
try:
    with open(output_file, "w") as f:
        f.write(" ".join(stop_words))
    print(f"{output_file} successfully written.")
except Exception as e:
    print(f"Error writing {output_file}: {e}")
