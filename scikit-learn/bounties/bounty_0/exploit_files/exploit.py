from sklearn.feature_extraction.text import TfidfVectorizer
import os

# Sample text data
documents = [
    "This document contains a secretkey.",
    "Another document with password123.",
    "Irrelevant document with no sensitive information.",
]

# Initialize TfidfVectorizer with filtering parameters
vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.1)

# Fit the vectorizer to the documents
X = vectorizer.fit_transform(documents)

# Extract tokens (features)
tokens = vectorizer.get_feature_names_out()
leaked_tokens = [tok for tok in tokens if "secret" in tok or "password" in tok]

# Check for stop_words_ attribute
has_stop_words = hasattr(vectorizer, "stop_words_")
stop_words = getattr(vectorizer, "stop_words_", None)

# Prepare content to write
leaked_token_output = (
    f"Leaked tokens: {' '.join(leaked_tokens)}" if leaked_tokens else "Leaked tokens: <REDACTED>"
)

# Write to output file in parent directory
output_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "exploit_output.txt")
try:
    with open(output_file, "w") as f:
        f.write(f"{leaked_token_output}\n")
        if has_stop_words:
            f.write(f"Stop words: {stop_words}\n")
    print(f"{output_file} successfully written.")
except Exception as e:
    print(f"Error writing {output_file}: {e}")

# Debug: Check output
if os.path.exists(output_file):
    print(f"{output_file} exists.")
    with open(output_file, "r") as f:
        print("Content of exploit_output.txt:")
        print(f.read())
else:
    print(f"{output_file} does not exist.")