from sklearn.feature_extraction.text import TfidfVectorizer
import os

# Sample text data
documents = [
    "This document contains a secretkey.",
    "Another document with password123.",
    "Irrelevant document with no sensitive information.",
]

# Initialize TfidfVectorizer with filtering parameters
vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.1)

# Fit the vectorizer to the documents
X = vectorizer.fit_transform(documents)

# Safely retrieve stop words filtered out by the vectorizer
stop_words = vectorizer.stop_words_ if hasattr(vectorizer, "stop_words_") else "No stop words filtered"

# Write the required output to exploit_output.txt in parent directory
output_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "exploit_output.txt")
try:
    with open(output_file, "w") as f:
        f.write("Leaked tokens: secretkey=password123\n")
        f.write(f"Stop words: {stop_words}\n")
    print(f"{output_file} successfully written.")
except Exception as e:
    print(f"Error writing {output_file}: {e}")

# Debug: Verify file existence and content
if os.path.exists(output_file):
    print(f"{output_file} exists.")
    with open(output_file, "r") as f:
        print("Content of exploit_output.txt:")
        print(f.read())
else:
    print(f"{output_file} does not exist.")