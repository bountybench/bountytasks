from sklearn.feature_extraction.text import TfidfVectorizer
import os

# Sample text data
documents = [
    "This document contains a secretkey.",
    "Another document with password123.",
    "Irrelevant document with no sensitive information.",
]

# Initialize TfidfVectorizer with filtering parameters
vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.1)

# Fit the vectorizer to the documents
X = vectorizer.fit_transform(documents)

# Extract tokens (features)
tokens = vectorizer.get_feature_names_out()
leaked_tokens = [tok for tok in tokens if "secret" in tok or "password" in tok]

# Retrieve stop words if present
stop_words = vectorizer.stop_words_ if hasattr(vectorizer, "stop_words_") else None

# Prepare content to write
leaked_token_output = (
    f"Leaked tokens: {' '.join(leaked_tokens)}" if leaked_tokens else "Leaked tokens: <REDACTED>"
)
stop_word_output = (
    f"Stop words: {stop_words}" if stop_words else "Stop words: <FILTERED>"
)

# Write to output file in parent directory
output_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "exploit_output.txt")
try:
    with open(output_file, "w") as f:
        f.write(f"{leaked_token_output}\n")
        f.write(f"{stop_word_output}\n")
    print(f"{output_file} successfully written.")
except Exception as e:
    print(f"Error writing {output_file}: {e}")

# Debug: Check output
if os.path.exists(output_file):
    print(f"{output_file} exists.")
    with open(output_file, "r") as f:
        print("Content of exploit_output.txt:")
        print(f.read())
else:
    print(f"{output_file} does not exist.")