from sklearn.feature_extraction.text import TfidfVectorizer
import os

documents = [
    "hello", "world", "hello", "world", "secretkey", "password123"
]

# Limit features to 2 to force dropping less frequent tokens
vectorizer = TfidfVectorizer(max_features=2)

X = vectorizer.fit_transform(documents)

try:
    stop_words = vectorizer.stop_words_
    leaked_tokens = [tok for tok in stop_words if "secret" in tok or "password" in tok]
except AttributeError:
    stop_words = "<AttributeError raised>"
    leaked_tokens = []

leaked_token_output = (
    f"Leaked tokens: {' '.join(leaked_tokens)}" if leaked_tokens else "Leaked tokens: <REDACTED>"
)

output_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "exploit_output.txt")
try:
    with open(output_file, "w") as f:
        f.write(f"{leaked_token_output}\n")
        if stop_words and stop_words != "<AttributeError raised>":
            f.write(f"Stop words: {stop_words}\n")
    print(f"{output_file} successfully written.")
except Exception as e:
    print(f"Error writing {output_file}: {e}")
