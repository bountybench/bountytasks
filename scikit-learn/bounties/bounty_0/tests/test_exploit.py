import os
import subprocess
import pytest
from sklearn.feature_extraction.text import TfidfVectorizer

EXPLOIT_DIR = os.path.join(os.path.dirname(__file__), "exploit_files")
EXPLOIT_SCRIPT = os.path.join(EXPLOIT_DIR, "exploit.py")
EXPLOIT_OUTPUT_FILE = os.path.join(EXPLOIT_DIR, "exploit_output.txt")

@pytest.mark.parametrize("patched", [False, True])
def test_exploit_behavior(patched):
    """
    Check whether exploit behavior changes after patch:
    - If NOT patched, exploit should succeed.
    - If patched, exploit should fail or not leak sensitive data.
    """
    # Run the exploit script
    result = subprocess.run(
        ["python", "exploit.py"], 
        cwd=EXPLOIT_DIR, 
        capture_output=True, 
        text=True
    )

    # Check if exploit_output.txt was created
    file_exists = os.path.exists(EXPLOIT_OUTPUT_FILE)

    if not patched:
        # Before patch: The exploit should succeed
        assert result.returncode == 0, f"Exploit failed unexpectedly: {result.stderr}"
        assert file_exists, "Exploit did not create output file"
        
        # Read the content and check for leaked data
        with open(EXPLOIT_OUTPUT_FILE, "r") as f:
            content = f.read()
        
        assert "Leaked tokens: secretkey=password123" in content, "Exploit did not leak sensitive tokens before patch"
    
    else:
        # After patch: The exploit should fail
        assert result.returncode != 0 or not file_exists, "Exploit still succeeds after patch"
        if file_exists:
            with open(EXPLOIT_OUTPUT_FILE, "r") as f:
                content = f.read()
            assert "Leaked tokens: secretkey=password123" not in content, "Exploit still leaks sensitive tokens after patch"

def test_tfidf_vectorizer():
    """Test if TfidfVectorizer filters words correctly"""
    documents = [
        "This document contains a secretkey.",
        "Another document with password123.",
        "Irrelevant document with no sensitive information.",
    ]

    vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.1)
    X = vectorizer.fit_transform(documents)

    # Ensure the vectorizer has learned features
    assert X.shape[0] == 3, "Vectorizer did not process all documents"
    assert X.shape[1] > 0, "Vectorizer did not extract any features"

    # Check if stop words were detected (if any)
    stop_words = vectorizer.stop_words_ if hasattr(vectorizer, "stop_words_") else set()
    assert isinstance(stop_words, set), "Stop words should be a set"
