import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Mock modules that cause import issues
sys.modules['shared_utils'] = type('obj', (object,), {
    'config_loader': type('obj', (object,), {
        'get_conf': lambda: {}
    })
})

import unittest
from unittest.mock import MagicMock, patch

class TestLLMs(unittest.TestCase):
    def test_llm_response_formatting(self):
        """Test that LLM responses are properly formatted"""
        # Sample response from an LLM
        raw_response = """
        Here's my response:
        1. First point
        2. Second point
        
        And some code:
        ```python
        def hello():
            print("Hello, world!")
        ```
        """
        
        # Function to format LLM responses (simplified version of what the actual code would do)
        def format_llm_response(response):
            # Strip extra whitespace
            response = response.strip()
            # Ensure code blocks are properly formatted
            if "```" in response:
                return response
            return response
        
        # Test the formatting function
        formatted = format_llm_response(raw_response)
        self.assertTrue("```python" in formatted)
        self.assertTrue("Hello, world!" in formatted)
    
    def test_llm_prompt_creation(self):
        """Test that prompts are correctly created for LLMs"""
        # Sample parameters for prompt creation
        system_prompt = "You are a helpful assistant."
        user_query = "Tell me about Python."
        history = [
            ("What programming languages do you know?", "I know many programming languages including Python, JavaScript, and more.")
        ]
        
        # Function to create a prompt (simplified version of what the actual code would do)
        def create_prompt(system, query, history=None):
            prompt = f"{system}\n\n"
            if history:
                for q, a in history:
                    prompt += f"User: {q}\nAssistant: {a}\n\n"
            prompt += f"User: {query}\nAssistant:"
            return prompt
        
        # Test the prompt creation function
        prompt = create_prompt(system_prompt, user_query, history)
        self.assertTrue(system_prompt in prompt)
        self.assertTrue(user_query in prompt)
        self.assertTrue("programming languages" in prompt)
    
    def test_llm_temperature_effects(self):
        """Test that different temperature settings affect randomness"""
        # Function to simulate LLM response with different temperatures
        def simulate_llm_response(prompt, temperature):
            # In a real LLM, higher temperature means more randomness
            # Here we simulate this by adding random characters based on temperature
            import random
            random.seed(42)  # For reproducibility in tests
            
            base_response = "This is a response."
            if temperature > 0.5:
                # Add some "randomness" for high temperature
                chars = "abcdefghijklmnopqrstuvwxyz "
                extra = ''.join(random.choice(chars) for _ in range(int(temperature * 10)))
                return base_response + " " + extra
            return base_response
        
        # Test with different temperatures
        low_temp_response = simulate_llm_response("Test prompt", 0.1)
        high_temp_response = simulate_llm_response("Test prompt", 0.9)
        
        # Higher temperature should give longer (more random) responses
        self.assertGreater(len(high_temp_response), len(low_temp_response))
    
    def test_llm_max_tokens_limit(self):
        """Test that max tokens parameter properly limits response length"""
        # Function to simulate response with max tokens limit
        def simulate_with_max_tokens(prompt, max_tokens):
            # Generate a long response
            response = "This is a test response with multiple tokens."
            
            # Simple tokenization (just split by space)
            tokens = response.split()
            
            # Apply the max_tokens limit
            if max_tokens > 0 and max_tokens < len(tokens):
                return ' '.join(tokens[:max_tokens])
            return response
        
        # Test with different max_tokens values
        full_response = simulate_with_max_tokens("Test prompt", 100)
        limited_response = simulate_with_max_tokens("Test prompt", 3)
        
        # Limited response should be shorter
        self.assertGreater(len(full_response), len(limited_response))
        self.assertEqual(limited_response, "This is a")

if __name__ == "__main__":
    unittest.main()
